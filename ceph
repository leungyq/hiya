#装好ceph的yum仓库
node1 { 
#修改配置文件/etc/hosts
cat /etc/hosts
192.168.4.50 client
192.168.4.51 node1
192.168.4.52 node2
192.168.4.53 node3
#注意！！！全部机子的/etc/hosts的配置文件内容都要一样
ssh-keygen -f /root/.ssh/id_rsa -N ''
ssh-copy-id 192.168.4.50  51  52  53
}
client {
在client机子上安装时间服务 chrony 并修改其配置文件 cat /etc/chrony.conf
server 0.centos.pool.ntp.org iburst
allow 192.168.4.0/24
local stratum 10
systemctl restart chronyd
}
node[1-3] {
让其他机子与clinet机子进行时间同步
cat /etc/chrony.conf
server 192.168.4.50 iburst
systemctl restart chronyd
}
准备好足够的磁盘空间
lsblk
vdb           252:16   0   10G  0 disk 
vdc           252:32   0   10G  0 disk 
vdd           252:48   0   10G  0 disk
node1 {
安装 ceph-deploy
创建目录给ceph部署集群并cd到该目录下，因为接下来基本所有操作都是在该目录进行
ceph-deploy new node[1-3]         cd进去后首先创建ceph集群配置
ceph-deploy install node[1-3]     给所有节点安装软件包
ceph-deploy mon create-initial    初始化所有节点的mon服务
到这里如果有报错多数是属于/etc/hosts没有全部一致！ ########################################################################
给准备好的磁盘分区,并且给分好的区归属所属者跟所属主(安装软件包时自动会生成ceph组)
parted /dev/vdb mklabel gpt
parted /dev/vdb mkpart primary 1M 50%
parted /dev/vdb mkpart primary 50%  100%
chown ceph.ceph /dev/vdb1
chown ceph.ceph /dev/vdb2
所有node机子都执行一遍上面五条命令(分出来的两个小分区是用来做存储服务器的日志journal盘)
初始化磁盘数据（仅node1操作即可）
ceph-deploy disk zap node1:vdc  node1:vdd
ceph-deploy disk zap node2:vdc  node2:vdd
ceph-deploy disk zap node3:vdc  node3:vdd
创建OSD存储空间（仅node1操作）
ceph-deploy osd create node1:vdc:/dev/vdb1 node1:vdd/dev/vdb2
ceph-deploy osd create node2:vdc:/dev/vdb1 node2:vdd/dev/vdb2
ceph-deploy osd create node3:vdc:/dev/vdb1 node3:vdd/dev/vdb3
ceph -s  验证测试
如果报clock skew的错误 看下时间同步，如果还是失败状态重启下exph相关服务
ceph osd lspools   查看以下创建好的存储池
rbd create demo-image(镜像名，随意) --image-feature layering --size 10G     创建第一个镜像
rbd create rbd/image --image-feature layering --size 10G   创建第二个镜像并指定创建在rbd池
rbd list   查看所有镜像
rbd info demo-image  查看镜像的状态
lsblk
rbd0          251:0    0   10G  0 disk
mkfs.ext4 /dev/rbd0
mount /dev/rbd0 /mnt  格式化并挂载
}
client {
需要安装 ecph-common 软件包
scp 192.168.4.51:/etc/ceph/ceph.conf  /etc/ceph/     #拷贝配置文件（否则不知道集群在哪）
scp 192.168.4.51:/etc/ceph/ceph.client.admin.keyring   /etc/ceph/  #拷贝连接密钥（否则无连接权限）
rbd map image
lsblk
rbd showmapped
mkfs.ext4 /dev/rbd0
mount /dev/rbd0 /mnt/
echo hiya > /mnt/test.txt
已经挂载到client机上
}
node1 { 
rbd snap create image --snap image-snap1  创建镜像快照
rbd snap ls image
#创建快照是为了当源文件被误删时可以恢复
}
rm -rf /mnt/test.txt
rbd snap rollback image --snap image-snap1  #在node1执行还原操作
client {
umount /mnt
mount /dev/rbd0 /mnt/
ls /mnt
#卸载重新挂载即可看到源文件
}
###########################################################################################################################
rbd snap protect image --snap image-snap1    #给image-snap1加保护  unprotect 取消保护
rbd clone image --snap image-snap1 image-clone --image-feature layering  #使用image的快照image-snap1克隆一个新的image-clone镜像
rbd info image-clone  #查看克隆镜像与父镜像快照的关系
rbd flatten image-clone
rbd info image-clone
client 撤销磁盘映射 {
umount /mnt
rbd showmapped
rbd unmap /dev/rbd/{poolname}/{imagename}
rbd unmap /dev/rbd/rbd/image
}
node1 删除快照与镜像 {
rbd snap rm image --snap image-snap
rbd list
rbd rm image
}
node1 缩小or扩大容量 {
缩小
rbd resize --size 7G image --allow-shrink
扩大
rbd resize --size 15G image
}








