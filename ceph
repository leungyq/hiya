#装好ceph的yum仓库
node1 { 
#修改配置文件/etc/hosts
cat /etc/hosts
192.168.4.50 client
192.168.4.51 node1
192.168.4.52 node2
192.168.4.53 node3
#注意！！！全部机子的/etc/hosts的配置文件内容都要一样
ssh-keygen -f /root/.ssh/id_rsa -N ''
ssh-copy-id 192.168.4.50  51  52  53
}
client {
在client机子上安装时间服务 chrony 并修改其配置文件 cat /etc/chrony.conf
server 0.centos.pool.ntp.org iburst
allow 192.168.4.0/24
local stratum 10
systemctl restart chronyd
}
node[1-3] {
让其他机子与clinet机子进行时间同步
cat /etc/chrony.conf
server 192.168.4.50 iburst
systemctl restart chronyd
}
准备好足够的磁盘空间
lsblk
vdb           252:16   0   10G  0 disk 
vdc           252:32   0   10G  0 disk 
vdd           252:48   0   10G  0 disk
node1 {
安装 ceph-deploy
创建目录给ceph部署集群并cd到该目录下，因为接下来基本所有操作都是在该目录进行
ceph-deploy new node[1-3]         cd进去后首先创建ceph集群配置
ceph-deploy install node[1-3]     给所有节点安装软件包
ceph-deploy mon create-initial    初始化所有节点的mon服务
到这里如果有报错多数是属于/etc/hosts没有全部一致！ ########################################################################
给准备好的磁盘分区,并且给分好的区归属所属者跟所属主(安装软件包时自动会生成ceph组)
parted /dev/vdb mklabel gpt
parted /dev/vdb mkpart primary 1M 50%
parted /dev/vdb mkpart primary 50%  100%
chown ceph.ceph /dev/vdb1
chown ceph.ceph /dev/vdb2
所有node机子都执行一遍上面五条命令(分出来的两个小分区是用来做存储服务器的日志journal盘)
初始化磁盘数据（仅node1操作即可）
ceph-deploy disk zap node1:vdc  node1:vdd
ceph-deploy disk zap node2:vdc  node2:vdd
ceph-deploy disk zap node3:vdc  node3:vdd
创建OSD存储空间（仅node1操作）
ceph-deploy osd create node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2
ceph-deploy osd create node2:vdc:/dev/vdb1 node2:vdd:/dev/vdb2
ceph-deploy osd create node3:vdc:/dev/vdb1 node3:vdd:/dev/vdb3
ceph -s  验证测试
如果报clock skew的错误 看下时间同步，如果还是失败状态重启下exph相关服务
ceph osd lspools   查看以下创建好的存储池
rbd create demo-image(镜像名，随意) --image-feature layering --size 10G     创建第一个镜像
rbd create rbd/image --image-feature layering --size 10G   创建第二个镜像并指定创建在rbd池
rbd list   查看所有镜像
rbd info demo-image  查看镜像的状态
lsblk
rbd0          251:0    0   10G  0 disk
mkfs.ext4 /dev/rbd0
mount /dev/rbd0 /mnt  格式化并挂载
}
client {
需要安装 ecph-common 软件包
scp 192.168.4.51:/etc/ceph/ceph.conf  /etc/ceph/     #拷贝配置文件（否则不知道集群在哪）
scp 192.168.4.51:/etc/ceph/ceph.client.admin.keyring   /etc/ceph/  #拷贝连接密钥（否则无连接权限）
rbd map image
lsblk
rbd showmapped
mkfs.ext4 /dev/rbd0
mount /dev/rbd0 /mnt/
echo hiya > /mnt/test.txt
已经挂载到client机上
}
node1 { 
rbd snap create image --snap image-snap1  创建镜像快照
rbd snap ls image
#创建快照是为了当源文件被误删时可以恢复
}
rm -rf /mnt/test.txt
rbd snap rollback image --snap image-snap1  #在node1执行还原操作
client {
umount /mnt
mount /dev/rbd0 /mnt/
ls /mnt
#卸载重新挂载即可看到源文件
}
###########################################################################################################################
rbd snap protect image --snap image-snap1    #给image-snap1加保护  unprotect 取消保护
rbd clone image --snap image-snap1 image-clone --image-feature layering  #使用image的快照image-snap1克隆一个新的image-clone镜像
rbd info image-clone  #查看克隆镜像与父镜像快照的关系
rbd flatten image-clone
rbd info image-clone
client 撤销磁盘映射 {
umount /mnt
rbd showmapped
rbd unmap /dev/rbd/{poolname}/{imagename}
rbd unmap /dev/rbd/rbd/image
}
node1 删除快照与镜像 {
rbd snap rm image --snap image-snap
rbd list
rbd rm image
}
node1 缩小or扩大容量 {
缩小
rbd resize --size 7G image --allow-shrink
扩大
rbd resize --size 15G image
}
错误的集合#########################################################################
[ceph_deploy][ERROR ] RuntimeError: ceph.client.admin.keyring not found
该错误是ceph.client.admin.keyring找不到文件
cepj-deploy admin node[1-4]  
传输keyring文件即可
############################################################################
[root@node81 ceph]# ceph -s
    cluster a5c7bf13-9f51-4366-b3b0-baad186cacda
     health HEALTH_WARN
            too few PGs per OSD (24 < min 30)
     monmap e1: 1 mons at {node84=192.168.4.84:6789/0}
            election epoch 3, quorum 0 node84
     osdmap e43: 8 osds: 8 up, 8 in
            flags sortbitwise
      pgmap v82: 64 pgs, 1 pools, 0 bytes data, 0 objects
            271 MB used, 81560 MB / 81831 MB avail
                  64 active+clean
该错误是pgp跟pg不平衡导致：
从上面可以看到，提示说每个osd上的pg数量小于最小的数目30个。
pgs为64，因为是3副本的配置，所以当有9个osd的时候，每个osd上均分了64/9 *3=21个pgs,也就是出现了如上的错误 小于最小配置30个。
解决办法：修改默认pool rbd的pg
ceph osd pool set rbd pg_num 128
ceph osd pool set rbd pgp_num 128
pg = pgp 即可
这里是简单的实验，pool上也没有数据，所以修改pg影响并不大，但是如果是生产环境，这时候再重新修改pg数，会对生产环境产生较大影响。
因为pg数变了，就会导致整个集群的数据重新均衡和迁移，数据越大响应io的时间会越长。所以，最好在一开始就设置好pg数



